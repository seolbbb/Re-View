import cv2
import os
import numpy as np
from datetime import timedelta

# MediaPipe Import (Optional)
try:
    import mediapipe as mp
    MP_AVAILABLE = True
except ImportError:
    MP_AVAILABLE = False
    print("âš  MediaPipe not found. Human removal will be disabled.")

class VideoProcessor:
    def __init__(self):
        if MP_AVAILABLE:
            self.mp_selfie_segmentation = mp.solutions.selfie_segmentation
            self.segmentation = self.mp_selfie_segmentation.SelfieSegmentation(model_selection=1) # 0: general, 1: landscape(faster)
            print("âœ… MediaPipe Selfie Segmentation loaded.")
        else:
            self.segmentation = None

    def extract_keyframes(self, video_path, output_dir='captured_frames', threshold=30, min_interval=2.0, capture_duration=3.0):
        """
        ë¹„ë””ì˜¤ì—ì„œ ì¥ë©´ ì „í™˜ì„ ê°ì§€í•˜ì—¬ í‚¤í”„ë ˆì„ ì¶”ì¶œ (Temporal Reconstruction + Human Removal)
        """
        if not os.path.exists(video_path):
            print(f"âŒ Video file not found: {video_path}")
            return []

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"ğŸ“‚ Created output directory: {output_dir}")

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print("âŒ Failed to open video.")
            return []

        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        
        print(f"ğŸ¬ Video Info: {duration:.2f}s, {fps:.2f} fps, {total_frames} frames")
        print(f"âš™ï¸ Settings: Threshold={threshold}, Min Interval={min_interval}s, Capture Duration={capture_duration}s")

        keyframes = []
        prev_frame_gray = None
        last_capture_time = -min_interval
        
        frame_idx = 0
        captured_count = 0

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            current_time = frame_idx / fps
            
            # 1. ì²« í”„ë ˆì„ ì²˜ë¦¬
            if frame_idx == 0:
                # ì²« ì¥ë©´ë„ 3ì´ˆê°„ ë¶„ì„í•˜ì—¬ ê¹¨ë—í•˜ê²Œ ì¶”ì¶œ
                clean_frame = self._collect_and_reconstruct(cap, frame_idx, duration_sec=capture_duration, fps=fps)
                if clean_frame is not None:
                    final_frame = self._remove_human(clean_frame)
                    self._save_frame(final_frame, current_time, output_dir, keyframes)
                    last_capture_time = current_time
                
                prev_frame_gray = cv2.cvtColor(cv2.resize(frame, (640, 360)), cv2.COLOR_BGR2GRAY)
                frame_idx += 1
                continue

            # 2. ìµœì†Œ ê°„ê²© ì²´í¬
            if current_time - last_capture_time < min_interval:
                frame_idx += 1
                continue

            # 3. ì¥ë©´ ì „í™˜ ê°ì§€
            curr_frame_small = cv2.resize(frame, (640, 360))
            curr_frame_gray = cv2.cvtColor(curr_frame_small, cv2.COLOR_BGR2GRAY)

            diff = cv2.absdiff(curr_frame_gray, prev_frame_gray)
            mean_diff = np.mean(diff)

            # 4. ì„ê³„ê°’ ì´ˆê³¼ ì‹œ -> Temporal Reconstruction -> Human Removal
            if mean_diff > threshold:
                print(f"ğŸ“¸ Scene Change Detected at {current_time:.2f}s (Diff: {mean_diff:.2f})")
                
                current_pos = cap.get(cv2.CAP_PROP_POS_FRAMES)
                
                # 3ì´ˆê°„ì˜ ë°ì´í„°ë¥¼ ëª¨ì•„ì„œ ë°°ê²½ ë³µì›
                clean_frame = self._collect_and_reconstruct(cap, current_pos, duration_sec=capture_duration, fps=fps)
                
                if clean_frame is not None:
                    final_frame = self._remove_human(clean_frame)
                    self._save_frame(final_frame, current_time, output_dir, keyframes)
                
                last_capture_time = current_time
                prev_frame_gray = curr_frame_gray
                captured_count += 1
                
                # ìº¡ì²˜ ë¶„ì„ì„ ìœ„í•´ ì´ë™í–ˆë˜ ìœ„ì¹˜ ë³µêµ¬
                cap.set(cv2.CAP_PROP_POS_FRAMES, current_pos)

            frame_idx += 1

        cap.release()
        
        # ì¤‘ë³µ ì œê±° ìˆ˜í–‰
        print(f"ğŸ” Removing duplicates (Initial: {len(keyframes)} frames)...")
        unique_keyframes = self._remove_duplicates(keyframes)
        
        print(f"âœ… Extraction complete. {len(unique_keyframes)} unique frames captured.")
        return unique_keyframes

    def _remove_duplicates(self, keyframes, hash_threshold=10):
        """
        dHashë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ í”„ë ˆì„ ì œê±°
        """
        if not keyframes:
            return []

        unique_frames = []
        last_hash = None
        removed_count = 0

        for item in keyframes:
            image_path = item['image_path']
            if not os.path.exists(image_path):
                continue
            
            # ì´ë¯¸ì§€ ë¡œë“œ (dHash ê³„ì‚°ìš©)
            img = cv2.imread(image_path)
            if img is None:
                continue
                
            curr_hash = self._compute_dhash(img)
            
            is_duplicate = False
            if last_hash is not None:
                # Hamming Distance ê³„ì‚°
                dist = bin(last_hash ^ curr_hash).count('1')
                if dist <= hash_threshold:
                    is_duplicate = True
            
            if is_duplicate:
                # ì¤‘ë³µì´ë©´ íŒŒì¼ ì‚­ì œ ë° ë¦¬ìŠ¤íŠ¸ ì œì™¸
                try:
                    os.remove(image_path)
                    removed_count += 1
                except OSError:
                    pass
            else:
                unique_frames.append(item)
                last_hash = curr_hash
        
        print(f"ğŸ—‘ Removed {removed_count} duplicate frames.")
        return unique_frames

    def _compute_dhash(self, image):
        """
        ì´ë¯¸ì§€ì˜ dHash (Difference Hash) ê³„ì‚°
        """
        # 1. 9x8ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ê°€ë¡œ 9, ì„¸ë¡œ 8)
        resized = cv2.resize(image, (9, 8))
        # 2. í‘ë°± ë³€í™˜
        gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)
        # 3. ì¸ì ‘ í”½ì…€ ë¹„êµ (ê°€ë¡œ ë°©í–¥)
        # í”½ì…€[i] < í”½ì…€[i+1] ì´ë©´ 1, ì•„ë‹ˆë©´ 0
        hash_val = 0
        for row in range(8):
            for col in range(8):
                if gray[row, col] < gray[row, col+1]:
                    hash_val |= 1 << (row * 8 + col)
        return hash_val

    def _remove_human(self, frame):
        """
        MediaPipeë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ëŒ ì˜ì—­ì„ ê°ì§€í•˜ê³  Inpaintingìœ¼ë¡œ ì œê±°
        """
        if self.segmentation is None:
            return frame

        # MediaPipeëŠ” RGB ì…ë ¥ì„ ë°›ìŒ
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.segmentation.process(frame_rgb)

        if results.segmentation_mask is None:
            return frame

        # ë§ˆìŠ¤í¬ ìƒì„± (ì‚¬ëŒì¸ ë¶€ë¶„: True, ë°°ê²½: False)
        # threshold 0.5 ì´ìƒì„ ì‚¬ëŒìœ¼ë¡œ íŒë‹¨
        condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.5
        
        # ì‚¬ëŒ ì˜ì—­ ë§ˆìŠ¤í¬ (uint8, 0 or 255)
        mask = (results.segmentation_mask > 0.5).astype(np.uint8) * 255
        
        # ë§ˆìŠ¤í¬ íŒ½ì°½ (Dilate) - ê²½ê³„ì„  ê¹”ë”í•˜ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´
        kernel = np.ones((5, 5), np.uint8)
        mask = cv2.dilate(mask, kernel, iterations=2)

        # Inpainting (Telea ì•Œê³ ë¦¬ì¦˜)
        # radius: ë³µì› ë°˜ê²½ (í´ìˆ˜ë¡ ë­‰ê°œì§ ì‹¬í•¨, ì‘ìœ¼ë©´ ëœ ì§€ì›Œì§)
        inpainted_frame = cv2.inpaint(frame, mask, 3, cv2.INPAINT_TELEA)

        return inpainted_frame

    def _collect_and_reconstruct(self, cap, start_pos, duration_sec=3.0, fps=30.0):
        """
        ì§€ì •ëœ ìœ„ì¹˜ë¶€í„° ì¼ì • ì‹œê°„ ë™ì•ˆì˜ í”„ë ˆì„ì„ ìˆ˜ì§‘í•˜ì—¬ Temporal Medianìœ¼ë¡œ ë°°ê²½ ë³µì›
        """
        frames = []
        original_pos = cap.get(cv2.CAP_PROP_POS_FRAMES)
        
        # 5ì´ˆ ë™ì•ˆ ìˆ˜ì§‘í•˜ë˜, 2í”„ë ˆì„ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§ (ë°€ë„ ë†’ì„)
        # 30fps * 5s = 150 frames -> /2 = 75 frames
        sample_interval = 2 
        max_frames = int(duration_sec * fps)
        
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_pos)
        
        for i in range(0, max_frames, sample_interval):
            # í˜„ì¬ ìœ„ì¹˜ì—ì„œ ì½ê¸°
            ret, frame = cap.read()
            if not ret:
                break
            frames.append(frame)
            
            # ë‹¤ìŒ ìƒ˜í”Œ ìœ„ì¹˜ë¡œ ì í”„ (read()ê°€ 1í”„ë ˆì„ ì´ë™í–ˆìœ¼ë¯€ë¡œ interval-1 ë§Œí¼ ë” ì´ë™)
            if sample_interval > 1:
                curr = cap.get(cv2.CAP_PROP_POS_FRAMES)
                cap.set(cv2.CAP_PROP_POS_FRAMES, curr + sample_interval - 1)
        
        # ìœ„ì¹˜ ë³µêµ¬
        cap.set(cv2.CAP_PROP_POS_FRAMES, original_pos)
        
        if not frames:
            return None
            
        # Temporal Median ê³„ì‚°
        # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ uint8 ìœ ì§€
        stacked_frames = np.stack(frames, axis=0)
        median_frame = np.median(stacked_frames, axis=0).astype(dtype=np.uint8)
        
        return median_frame

    def _save_frame(self, frame, timestamp, output_dir, keyframes_list):
        """í”„ë ˆì„ ì €ì¥ ë° ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ í—¬í¼ í•¨ìˆ˜"""
        filename = f"frame_{timestamp:.2f}.jpg"
        filepath = os.path.join(output_dir, filename)
        cv2.imwrite(filepath, frame)
        
        keyframes_list.append({
            'timestamp': timestamp,
            'image_path': filepath
        })

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©)
    # data/input í´ë”ì— í…ŒìŠ¤íŠ¸ ì˜ìƒì´ ìˆë‹¤ê³  ê°€ì •
    video_file = os.path.join("data", "input", "dirty_ex2_masked.mp4")
    output_folder = os.path.join("data", "output", "captured_frames_masked")
    
    if not os.path.exists(video_file):
        print(f"âš  Test video not found: {video_file}")
        print("Please place a test video in 'data/input/' or update the path.")
    else:
        processor = VideoProcessor()
        # thresholdì™€ min_intervalì€ ì˜ìƒ íŠ¹ì„±ì— ë”°ë¼ ì¡°ì ˆ í•„ìš”
        processor.extract_keyframes(video_file, output_dir=output_folder, threshold=10, min_interval=2.0, capture_duration=5.0)

