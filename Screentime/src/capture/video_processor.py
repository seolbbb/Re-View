import cv2
import os
import numpy as np

class VideoProcessor:
    """
    ë¹„ë””ì˜¤ ì²˜ë¦¬ í´ë˜ìŠ¤: ê°•ì˜ ì˜ìƒì—ì„œ ìŠ¬ë¼ì´ë“œ ì „í™˜ì„ ê°ì§€í•˜ê³  í‚¤í”„ë ˆì„ì„ ì¶”ì¶œ
    
    [ì£¼ìš” ê¸°ëŠ¥]
    1. Scene Detection: í”„ë ˆì„ ê°„ í”½ì…€ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ì¥ë©´ ì „í™˜ ê°ì§€
    2. Keyframe Capture: ê°ì§€ëœ ì‹œì ì˜ ê¹¨ë—í•œ ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ ì €ì¥
    3. Mouse Removal: Temporal Median ê¸°ë²•ìœ¼ë¡œ ë§ˆìš°ìŠ¤ í¬ì¸í„° ì œê±°
    4. Duplicate Removal: dHash ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¤‘ë³µ í”„ë ˆì„ ì œê±°
    
    [ì‚¬ìš© ì˜ˆì‹œ]
    >>> processor = VideoProcessor()
    >>> keyframes = processor.extract_keyframes(
    ...     video_path="lecture.mp4",
    ...     output_dir="output/frames",
    ...     threshold=8,
    ...     min_interval=0.5
    ... )
    
    [í•µì‹¬ ì•Œê³ ë¦¬ì¦˜]
    - Temporal Median: ì‹œê°„ì ìœ¼ë¡œ ë¶„ì‚°ëœ í”„ë ˆì„ë“¤ì˜ ì¤‘ì•™ê°’ì„ ê³„ì‚°í•˜ì—¬
                       ì›€ì§ì´ëŠ” ë¬¼ì²´(ë§ˆìš°ìŠ¤)ëŠ” ì œê±°í•˜ê³  ê³ ì •ëœ ë°°ê²½(ìŠ¬ë¼ì´ë“œ)ë§Œ ì¶”ì¶œ
    - Multi-point Sampling: ìŠ¬ë¼ì´ë“œ ì „ì²´ êµ¬ê°„ì—ì„œ ë¬´ì‘ìœ„ë¡œ í”„ë ˆì„ì„ ìˆ˜ì§‘í•˜ì—¬
                           ë§ˆìš°ìŠ¤ê°€ ë‹¤ì–‘í•œ ìœ„ì¹˜ì— ìˆëŠ” ìˆœê°„ë“¤ì„ í™•ë³´
    """
    def __init__(self):
        pass

    def extract_keyframes(self, video_path, output_dir='captured_frames', threshold=30, min_interval=2.0, verbose=False):
        """
        [í•µì‹¬ ê¸°ëŠ¥] ë¹„ë””ì˜¤ì—ì„œ ì¥ë©´ ì „í™˜ì„ ê°ì§€í•˜ì—¬ í‚¤í”„ë ˆì„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            video_path (str): ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_dir (str): ì¶”ì¶œëœ ì´ë¯¸ì§€ê°€ ì €ì¥ë  í´ë”
            threshold (float): ì¥ë©´ ì „í™˜ ê°ì§€ ì„ê³„ê°’ (í”½ì…€ ì°¨ì´ í‰ê· , ë†’ì„ìˆ˜ë¡ ë‘”ê°)
            min_interval (float): ìº¡ì²˜ ê°„ ìµœì†Œ ì‹œê°„ ê°„ê²© (ì´ˆ ë‹¨ìœ„)
            verbose (bool): ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
            
        Returns:
            list: ìº¡ì²˜ëœ í”„ë ˆì„ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        if not os.path.exists(video_path):
            print(f"âŒ Video file not found: {video_path}")
            return []

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"ğŸ“‚ Created output directory: {output_dir}")

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print("âŒ Failed to open video.")
            return []

        # ë¹„ë””ì˜¤ ì •ë³´ ì¶œë ¥
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        
        print(f"ğŸ¬ Video Info: {duration:.2f}s, {fps:.2f} fps, {total_frames} frames")
        print(f"âš™ï¸ Settings: Threshold={threshold}, Min Interval={min_interval}s")

        keyframes = []
        prev_frame_gray = None  # ì´ì „ í”„ë ˆì„ (ì¥ë©´ ë¹„êµìš©)
        last_capture_time = -min_interval  # ë§ˆì§€ë§‰ ìº¡ì²˜ ì‹œê°„ (ì¤‘ë³µ ë°©ì§€)
        
        # === ìŠ¬ë¼ì´ë“œ ê²½ê³„ ì¶”ì  ===
        # slide_boundaries: ê° ìŠ¬ë¼ì´ë“œì˜ (ì‹œì‘ì‹œê°„, ì¢…ë£Œì‹œê°„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        # ì˜ˆ: [(0, 120), (120, 240), (240, 360)] = 3ê°œ ìŠ¬ë¼ì´ë“œ
        slide_boundaries = []
        last_scene_change = 0.0  # ë§ˆì§€ë§‰ ì¥ë©´ ì „í™˜ ì‹œì 
        
        frame_idx = 0
        captured_count = 0

        # === ë©”ì¸ ë£¨í”„: ëª¨ë“  í”„ë ˆì„ì„ ìˆœíšŒí•˜ë©° ì¥ë©´ ì „í™˜ ê°ì§€ ===
        while True:
            ret, frame = cap.read()
            if not ret:  # ë¹„ë””ì˜¤ ë
                break

            current_time = frame_idx / fps  # í˜„ì¬ í”„ë ˆì„ì˜ ì‹œê°„(ì´ˆ)
            
            # ============================================================
            # [Logic 1] ì²« í”„ë ˆì„ ì²˜ë¦¬
            # ============================================================
            # ì²« í”„ë ˆì„ì€ ë¬´ì¡°ê±´ ìº¡ì²˜ ëŒ€ìƒ (ì²« ë²ˆì§¸ ìŠ¬ë¼ì´ë“œ)
            if frame_idx == 0:
                last_scene_change = 0.0  # ì²« ìŠ¬ë¼ì´ë“œ ì‹œì‘ ì‹œì  ê¸°ë¡
                
                # ë‹¤ì¤‘ ì‹œì  ìƒ˜í”Œë§: 0ì´ˆ~6ì´ˆ êµ¬ê°„ì—ì„œ 30ê°œ í”„ë ˆì„ ëœë¤ ìˆ˜ì§‘
                # â†’ ë§ˆìš°ìŠ¤ê°€ ë‹¤ì–‘í•œ ìœ„ì¹˜ì— ìˆëŠ” ìˆœê°„ë“¤ì„ í™•ë³´í•˜ì—¬ Median ê³„ì‚°
                clean_frame = self._apply_temporal_median_multipoint(
                    cap, 0.0, min(6.0, duration), fps, num_samples=30
                )
                if clean_frame is not None:
                    self._save_frame(clean_frame, current_time, output_dir, keyframes)
                else:
                    self._save_frame(frame, current_time, output_dir, keyframes)
                
                last_capture_time = current_time
                
                # ë‹¤ìŒ í”„ë ˆì„ê³¼ ë¹„êµí•˜ê¸° ìœ„í•´ í˜„ì¬ í”„ë ˆì„ì„ í‘ë°±ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                # 640x360ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ â†’ ê³„ì‚° ì†ë„ í–¥ìƒ
                prev_frame_gray = cv2.cvtColor(cv2.resize(frame, (640, 360)), cv2.COLOR_BGR2GRAY)
                frame_idx += 1
                continue

            # ============================================================
            # [Logic 2] ìµœì†Œ ê°„ê²© ì²´í¬
            # ============================================================
            # ê°™ì€ ìŠ¬ë¼ì´ë“œ ë‚´ì—ì„œ ë„ˆë¬´ ìì£¼ ìº¡ì²˜í•˜ëŠ” ê²ƒì„ ë°©ì§€
            # ì˜ˆ: min_interval=0.5ì´ˆ â†’ 0.5ì´ˆ ì´ë‚´ì—ëŠ” ì¬ìº¡ì²˜ ì•ˆ í•¨
            if current_time - last_capture_time < min_interval:
                frame_idx += 1
                continue

            # ============================================================
            # [Logic 3] ì¥ë©´ ì „í™˜ ê°ì§€ (Pixel Difference)
            # ============================================================
            # ì´ì „ í”„ë ˆì„ê³¼ í˜„ì¬ í”„ë ˆì„ì˜ í”½ì…€ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ì¥ë©´ ì „í™˜ íŒë‹¨
            
            # Step 1: í˜„ì¬ í”„ë ˆì„ì„ ì‘ê²Œ ë¦¬ì‚¬ì´ì¦ˆ & í‘ë°± ë³€í™˜
            curr_frame_small = cv2.resize(frame, (640, 360))
            curr_frame_gray = cv2.cvtColor(curr_frame_small, cv2.COLOR_BGR2GRAY)

            # Step 2: ì´ì „ í”„ë ˆì„ê³¼ì˜ ì ˆëŒ€ ì°¨ì´ ê³„ì‚°
            # diff[y, x] = |current[y, x] - previous[y, x]|
            diff = cv2.absdiff(curr_frame_gray, prev_frame_gray)
            
            # Step 3: í‰ê·  ì°¨ì´ ê³„ì‚° (0~255 ë²”ìœ„)
            # mean_diffê°€ í´ìˆ˜ë¡ â†’ ë‘ í”„ë ˆì„ì´ ë§ì´ ë‹¤ë¦„ â†’ ì¥ë©´ ì „í™˜ ê°€ëŠ¥ì„± ë†’ìŒ
            mean_diff = np.mean(diff)

            # [ë””ë²„ê¹… ë¡œê·¸] verbose=Trueì¼ ë•Œ, ì„ê³„ê°’ì˜ ì ˆë°˜ ì´ìƒì¸ ë³€í™” ì¶œë ¥
            # â†’ ì–´ë–¤ ì‹œì ì—ì„œ ë³€í™”ê°€ ê°ì§€ë˜ëŠ”ì§€ í™•ì¸ ê°€ëŠ¥
            if verbose and mean_diff > (threshold / 2):
                print(f"   [Diff Check] Time: {current_time:.2f}s | Diff: {mean_diff:.2f} (Threshold: {threshold})")

            # ============================================================
            # [Logic 4] ì„ê³„ê°’ ì´ˆê³¼ ì‹œ â†’ ì¥ë©´ ì „í™˜ìœ¼ë¡œ íŒë‹¨í•˜ê³  ìº¡ì²˜
            # ============================================================
            if mean_diff > threshold:
                print(f"ğŸ“¸ Scene Change Detected at {current_time:.2f}s (Diff: {mean_diff:.2f})")
                
                # --- ë””ë²„ê¹…: ì›ë³¸ í”„ë ˆì„ ì €ì¥ ---
                # ë§ˆìš°ìŠ¤ ì œê±° ì „ì˜ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ debug í´ë”ì— ì €ì¥
                # â†’ ì–´ë–¤ ì¥ë©´ì´ ê°ì§€ë˜ì—ˆëŠ”ì§€, ë§ˆìš°ìŠ¤ ì œê±° ì „í›„ ë¹„êµ ê°€ëŠ¥
                debug_dir = os.path.join(output_dir, "debug_scene_changes")
                if not os.path.exists(debug_dir):
                    os.makedirs(debug_dir)
                cv2.imwrite(
                    os.path.join(debug_dir, f"scene_change_{current_time:.2f}s_diff_{mean_diff:.1f}.jpg"),
                    frame
                )
                
                # --- ìŠ¬ë¼ì´ë“œ ê²½ê³„ ê¸°ë¡ ---
                # ì´ì „ ì¥ë©´ ì „í™˜ ì‹œì  ~ í˜„ì¬ ì‹œì  = í•˜ë‚˜ì˜ ìŠ¬ë¼ì´ë“œ
                slide_boundaries.append((last_scene_change, current_time))
                
                # --- ìŠ¬ë¼ì´ë“œ ì •ë³´ ì¶œë ¥ ---
                slide_start = last_scene_change
                slide_end = current_time
                slide_duration = slide_end - slide_start
                print(f"   ğŸ“Š Slide boundary: [{slide_start:.1f}s ~ {slide_end:.1f}s] (Duration: {slide_duration:.1f}s)")
                
                # ============================================================
                # [ë§ˆìš°ìŠ¤ ì œê±°] ë‹¤ì¤‘ ì‹œì  ìƒ˜í”Œë§ vs ì–‘ë°©í–¥ ìˆ˜ì§‘
                # ============================================================
                # ìŠ¬ë¼ì´ë“œ ê¸¸ì´ì— ë”°ë¼ ë‹¤ë¥¸ ì „ëµ ì‚¬ìš©
                
                if slide_duration >= 3.0:
                    # --- ì „ëµ A: ë‹¤ì¤‘ ì‹œì  ìƒ˜í”Œë§ (ê¸´ ìŠ¬ë¼ì´ë“œ) ---
                    # ìŠ¬ë¼ì´ë“œ ì „ì²´ êµ¬ê°„ì—ì„œ ë¬´ì‘ìœ„ë¡œ 50ê°œ í”„ë ˆì„ ìˆ˜ì§‘
                    # ì¥ì : ë§ˆìš°ìŠ¤ê°€ ë‹¤ì–‘í•œ ìœ„ì¹˜ì— ìˆëŠ” ìˆœê°„ë“¤ì„ í™•ë³´
                    #      â†’ Median ê³„ì‚° ì‹œ ë§ˆìš°ìŠ¤ê°€ ì—†ëŠ” ë°°ê²½ë§Œ ì¶”ì¶œ
                    clean_frame = self._apply_temporal_median_multipoint(
                        cap, slide_start, slide_end, fps, num_samples=50
                    )
                else:
                    # --- ì „ëµ B: ì–‘ë°©í–¥ ìˆ˜ì§‘ (ì§§ì€ ìŠ¬ë¼ì´ë“œ) ---
                    # ì „í™˜ ì „ 2ì´ˆ + ì „í™˜ í›„ 4ì´ˆ = ì´ 6ì´ˆ ìˆ˜ì§‘
                    # ì§§ì€ ìŠ¬ë¼ì´ë“œëŠ” ì „ì²´ êµ¬ê°„ì´ ë¶€ì¡±í•˜ë¯€ë¡œ ì „í›„ êµ¬ê°„ í™œìš©
                    current_pos = cap.get(cv2.CAP_PROP_POS_FRAMES)
                    clean_frame = self._apply_temporal_median_bidirectional(
                        cap, current_pos, before_duration=2.0, after_duration=4.0, fps=fps
                    )
                
                # --- ë³µì›ëœ í”„ë ˆì„ ì €ì¥ ---
                if clean_frame is not None:
                    self._save_frame(clean_frame, current_time, output_dir, keyframes)
                else:
                    # ë³µì› ì‹¤íŒ¨ ì‹œ ì›ë³¸ í”„ë ˆì„ ì €ì¥ (fallback)
                    self._save_frame(frame, current_time, output_dir, keyframes)
                
                # --- ìƒíƒœ ì—…ë°ì´íŠ¸ ---
                last_capture_time = current_time  # ë§ˆì§€ë§‰ ìº¡ì²˜ ì‹œê°„ ê°±ì‹ 
                last_scene_change = current_time  # ë§ˆì§€ë§‰ ì¥ë©´ ì „í™˜ ì‹œì  ê°±ì‹ 
                prev_frame_gray = curr_frame_gray  # ë‹¤ìŒ ë¹„êµë¥¼ ìœ„í•œ í”„ë ˆì„ ê°±ì‹ 
                captured_count += 1

            frame_idx += 1

        # ============================================================
        # [ë§ˆì§€ë§‰ ìŠ¬ë¼ì´ë“œ ì²˜ë¦¬]
        # ============================================================
        # ë§ˆì§€ë§‰ ì¥ë©´ ì „í™˜ ~ ë¹„ë””ì˜¤ ë = ë§ˆì§€ë§‰ ìŠ¬ë¼ì´ë“œ
        if last_scene_change < duration:
            slide_boundaries.append((last_scene_change, duration))

        cap.release()
        
        print(f"ğŸ“‹ Total slides detected: {len(slide_boundaries)}")
        
        # [Logic 5] ì¤‘ë³µ ì œê±° (Post-processing)
        print(f"ğŸ” Removing duplicates (Initial: {len(keyframes)} frames)...")
        unique_keyframes = self._remove_duplicates_by_dhash(keyframes)
        
        print(f"âœ… Extraction complete. {len(unique_keyframes)} unique frames captured.")
        return unique_keyframes

    # ---------------------------------------------------------
    # [Helper Function] ì–‘ë°©í–¥ Temporal Median
    # ---------------------------------------------------------
    def _apply_temporal_median_bidirectional(self, cap, start_pos, before_duration=2.0, after_duration=4.0, fps=30.0):
        """
        ì–‘ë°©í–¥ Temporal Median: ì¥ë©´ ì „í™˜ ì „í›„ì˜ í”„ë ˆì„ì„ ìˆ˜ì§‘í•˜ì—¬ ë°°ê²½ ë³µì›
        
        Args:
            cap: VideoCapture ê°ì²´
            start_pos: ì¥ë©´ ì „í™˜ ê°ì§€ ì‹œì ì˜ í”„ë ˆì„ ìœ„ì¹˜
            before_duration: ì „í™˜ ì´ì „ êµ¬ê°„ ìˆ˜ì§‘ ì‹œê°„ (ì´ˆ)
            after_duration: ì „í™˜ ì´í›„ êµ¬ê°„ ìˆ˜ì§‘ ì‹œê°„ (ì´ˆ)
            fps: í”„ë ˆì„ë ˆì´íŠ¸
            
        Returns:
            ë³µì›ëœ ë°°ê²½ í”„ë ˆì„ (ë§ˆìš°ìŠ¤ ì œê±°ë¨)
        """
        frames = []
        original_pos = cap.get(cv2.CAP_PROP_POS_FRAMES)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # ìˆ˜ì§‘ ë²”ìœ„ ê³„ì‚°
        before_frames = int(before_duration * fps)
        after_frames = int(after_duration * fps)
        
        # ê²½ê³„ ì¡°ê±´ ì²˜ë¦¬: ì‹œì‘ ìœ„ì¹˜ê°€ ë¹„ë””ì˜¤ ì‹œì‘ ë¶€ë¶„ì´ë©´ before ìƒëµ
        collect_start = max(0, int(start_pos) - before_frames)
        collect_end = min(total_frames, int(start_pos) + after_frames)
        
        # ìƒ˜í”Œë§ ê°„ê²© (2í”„ë ˆì„ë§ˆë‹¤ 1ê°œ ìˆ˜ì§‘)
        sample_interval = 2
        
        # í”„ë ˆì„ ìˆ˜ì§‘
        cap.set(cv2.CAP_PROP_POS_FRAMES, collect_start)
        
        for frame_pos in range(collect_start, collect_end, sample_interval):
            ret, frame = cap.read()
            if not ret:
                break
            frames.append(frame)
            
            # ë‹¤ìŒ ìƒ˜í”Œ ìœ„ì¹˜ë¡œ ì´ë™
            if sample_interval > 1:
                curr = cap.get(cv2.CAP_PROP_POS_FRAMES)
                next_pos = curr + sample_interval - 1
                if next_pos < collect_end:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, next_pos)
        
        # ì›ë˜ ìœ„ì¹˜ ë³µêµ¬
        cap.set(cv2.CAP_PROP_POS_FRAMES, original_pos)
        
        if len(frames) < 3:  # ìµœì†Œ 3ê°œ í”„ë ˆì„ í•„ìš”
            return None
            
        # Temporal Median ê³„ì‚°
        stacked_frames = np.stack(frames, axis=0)
        median_frame = np.median(stacked_frames, axis=0).astype(dtype=np.uint8)
        
        return median_frame

    # ---------------------------------------------------------
    # [Helper Function] ë‹¤ì¤‘ ì‹œì  ìƒ˜í”Œë§ Temporal Median
    # ---------------------------------------------------------
    def _apply_temporal_median_multipoint(self, cap, start_time, end_time, fps, num_samples=50):
        """
        [ë‹¤ì¤‘ ì‹œì  ìƒ˜í”Œë§] ìŠ¬ë¼ì´ë“œ ì „ì²´ êµ¬ê°„ì—ì„œ ë¬´ì‘ìœ„ë¡œ í”„ë ˆì„ì„ ìˆ˜ì§‘í•˜ì—¬ ë°°ê²½ ë³µì›
        
        [í•µì‹¬ ì•„ì´ë””ì–´]
        - ë§ˆìš°ìŠ¤ëŠ” ì‹œê°„ì— ë”°ë¼ ìœ„ì¹˜ê°€ ë³€í•¨
        - ìŠ¬ë¼ì´ë“œ ì „ì²´ êµ¬ê°„ì—ì„œ ëœë¤í•˜ê²Œ í”„ë ˆì„ì„ ìˆ˜ì§‘í•˜ë©´,
          ê° í”½ì…€ ìœ„ì¹˜ì—ì„œ "ë§ˆìš°ìŠ¤ê°€ ì—†ëŠ” í”„ë ˆì„"ì´ ê³¼ë°˜ìˆ˜ê°€ ë¨
        - Temporal Median ê³„ì‚° ì‹œ ë§ˆìš°ìŠ¤ëŠ” ì‚¬ë¼ì§€ê³  ë°°ê²½(ìŠ¬ë¼ì´ë“œ)ë§Œ ë‚¨ìŒ
        
        [ì˜ˆì‹œ]
        ìŠ¬ë¼ì´ë“œ êµ¬ê°„: [120ì´ˆ ~ 240ì´ˆ] (120ì´ˆ ë™ì•ˆ)
        ë§ˆìš°ìŠ¤ ìœ„ì¹˜:
          - 120~130ì´ˆ: (100, 200)
          - 130~140ì´ˆ: (150, 250)
          - 140~150ì´ˆ: (200, 300)
          ...
        
        ëœë¤ ìƒ˜í”Œë§ 50ê°œ â†’ ê° í”½ì…€ì—ì„œ ë§ˆìš°ìŠ¤ê°€ ì—†ëŠ” ìˆœê°„ì´ ëŒ€ë¶€ë¶„
        â†’ Median ê²°ê³¼ = ë§ˆìš°ìŠ¤ ì—†ëŠ” ê¹¨ë—í•œ ìŠ¬ë¼ì´ë“œ
        
        Args:
            cap: VideoCapture ê°ì²´
            start_time: ìŠ¬ë¼ì´ë“œ ì‹œì‘ ì‹œê°„ (ì´ˆ)
            end_time: ìŠ¬ë¼ì´ë“œ ì¢…ë£Œ ì‹œê°„ (ì´ˆ)
            fps: í”„ë ˆì„ë ˆì´íŠ¸
            num_samples: ìˆ˜ì§‘í•  ìƒ˜í”Œ ê°œìˆ˜ (ê¸°ë³¸ 50ê°œ)
            
        Returns:
            ë³µì›ëœ ë°°ê²½ í”„ë ˆì„ (ë§ˆìš°ìŠ¤ ì œê±°ë¨) ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        frames = []
        original_pos = cap.get(cv2.CAP_PROP_POS_FRAMES)  # í˜„ì¬ ìœ„ì¹˜ ì €ì¥ (ë‚˜ì¤‘ì— ë³µêµ¬)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # === Step 1: ì‹œê°„(ì´ˆ) â†’ í”„ë ˆì„ ë²ˆí˜¸ ë³€í™˜ ===
        start_frame = int(start_time * fps)
        end_frame = int(end_time * fps)
        
        # === Step 2: ê²½ê³„ ì¡°ê±´ ì²˜ë¦¬ ===
        start_frame = max(0, start_frame)  # ìŒìˆ˜ ë°©ì§€
        end_frame = min(total_frames, end_frame)  # ë¹„ë””ì˜¤ ë ì´ˆê³¼ ë°©ì§€
        
        if end_frame - start_frame < 10:  # ìµœì†Œ 10í”„ë ˆì„ í•„ìš”
            return None
        
        # === Step 3: ë¬´ì‘ìœ„ í”„ë ˆì„ ìœ„ì¹˜ ìƒì„± ===
        np.random.seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ (ê°™ì€ ì˜ìƒì€ í•­ìƒ ê°™ì€ ê²°ê³¼)
        random_frames = np.random.randint(start_frame, end_frame, num_samples)
        random_frames = np.unique(random_frames)  # ì¤‘ë³µ ì œê±°
        random_frames.sort()  # ì •ë ¬ (ìˆœì°¨ ì ‘ê·¼ì´ ë¹ ë¦„)
        
        print(f"   ğŸ² Random sampling: {len(random_frames)} frames from [{start_time:.1f}s ~ {end_time:.1f}s]")
        
        # === Step 4: í”„ë ˆì„ ìˆ˜ì§‘ ===
        for frame_pos in random_frames:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_pos)  # í•´ë‹¹ í”„ë ˆì„ìœ¼ë¡œ ì´ë™
            ret, frame = cap.read()
            if not ret:
                continue
            frames.append(frame)
        
        # === Step 5: ì›ë˜ ìœ„ì¹˜ ë³µêµ¬ ===
        # ë©”ì¸ ë£¨í”„ê°€ ê³„ì† ì§„í–‰ë  ìˆ˜ ìˆë„ë¡ ì›ë˜ ìœ„ì¹˜ë¡œ ë˜ëŒë¦¼
        cap.set(cv2.CAP_PROP_POS_FRAMES, original_pos)
        
        if len(frames) < 3:  # ìµœì†Œ 3ê°œ í”„ë ˆì„ í•„ìš” (Median ê³„ì‚° ìœ„í•´)
            return None
            
        # === Step 6: Temporal Median ê³„ì‚° ===
        # ê° í”½ì…€ ìœ„ì¹˜ì—ì„œ ì¤‘ê°„ê°’(Median)ì„ ê³„ì‚°
        # ì˜ˆ: í”½ì…€ (100, 200)ì—ì„œ 50ê°œ í”„ë ˆì„ì˜ ê°’ì´ [10, 15, 200, 12, 14, ...]
        #     â†’ Median = 14 (ë§ˆìš°ìŠ¤ ê°’ 200ì€ ì´ìƒì¹˜ë¡œ ë¬´ì‹œë¨)
        stacked_frames = np.stack(frames, axis=0)  # (num_frames, height, width, 3)
        median_frame = np.median(stacked_frames, axis=0).astype(dtype=np.uint8)
        
        return median_frame

    # ---------------------------------------------------------
    # [Helper Function] ì¤‘ë³µ í”„ë ˆì„ ì œê±° (dHash)
    # ---------------------------------------------------------
    def _remove_duplicates_by_dhash(self, keyframes, hash_threshold=5):
        """
        dHash(Difference Hash)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ í”„ë ˆì„ ì œê±°
        """
        if not keyframes:
            return []

        unique_frames = []
        last_hash = None
        removed_count = 0

        for item in keyframes:
            image_path = item['image_path']
            if not os.path.exists(image_path):
                continue
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            img = cv2.imread(image_path)
            if img is None:
                continue
                
            curr_hash = self._calculate_dhash(img)
            
            is_duplicate = False
            if last_hash is not None:
                # Hamming Distance ê³„ì‚°
                dist = bin(last_hash ^ curr_hash).count('1')
                if dist <= hash_threshold:
                    is_duplicate = True
            
            if is_duplicate:
                try:
                    os.remove(image_path)
                    removed_count += 1
                except OSError:
                    pass
            else:
                unique_frames.append(item)
                last_hash = curr_hash
        
        print(f"ğŸ—‘ Removed {removed_count} duplicate frames.")
        return unique_frames

    # ---------------------------------------------------------
    # [Helper Function] dHash ê³„ì‚°
    # ---------------------------------------------------------
    def _calculate_dhash(self, image):
        """ì´ë¯¸ì§€ì˜ dHash (Difference Hash) ê³„ì‚°"""
        resized = cv2.resize(image, (9, 8))
        gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)
        hash_val = 0
        for row in range(8):
            for col in range(8):
                if gray[row, col] < gray[row, col+1]:
                    hash_val |= 1 << (row * 8 + col)
        return hash_val

    # ---------------------------------------------------------
    # [Helper Function] í”„ë ˆì„ ì €ì¥
    # ---------------------------------------------------------
    def _save_frame(self, frame, timestamp, output_dir, keyframes_list):
        """í”„ë ˆì„ ì €ì¥ í—¬í¼ í•¨ìˆ˜"""
        filename = f"frame_{timestamp:.2f}.jpg"
        filepath = os.path.join(output_dir, filename)
        cv2.imwrite(filepath, frame)
        
        keyframes_list.append({
            'timestamp': timestamp,
            'image_path': filepath
        })

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    video_file = os.path.join("data", "input", "dirty_ex2_masked.mp4")
    output_folder = os.path.join("data", "output", "captured_frames_masked")
    
    if not os.path.exists(video_file):
        print(f"âš  Test video not found: {video_file}")
    else:
        processor = VideoProcessor()
        processor.extract_keyframes(video_file, output_dir=output_folder, threshold=10, min_interval=2.0)
