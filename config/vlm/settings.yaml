model_name: qwen/qwen3-vl-32b-instruct
prompt_version: vlm_v1.1
request_params:
  temperature: 0.4          # 생성 다양성 조절 (낮을수록 더 보수적)
  top_p: 0.9                # 확률 분포 상위 p 누적값까지만 샘플링
  max_tokens: null          # 최대 생성 토큰 수 (null = 한도 내에서 가능한 만큼 생성)
  presence_penalty: 0.0     # 새로운 주제/단어를 얼마나 유도할지
  frequency_penalty: 0.1    # 동일 단어 반복 억제 정도
  stream: false             # 스트리밍 응답 여부 (false = 한 번에 결과 수신)
  stop:                     # 모델 출력 중단 토큰
    - "</s>"
    - "<|endoftext|>"
