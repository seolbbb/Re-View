import cv2
import os
import numpy as np

class VideoProcessor:
    """
    ë¹„ë””ì˜¤ ì²˜ë¦¬ í´ë˜ìŠ¤: ê°•ì˜ ì˜ìƒì—ì„œ ìŠ¬ë¼ì´ë“œ ì „í™˜ì„ ê°ì§€í•˜ê³  í‚¤í”„ë ˆì„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    [ì£¼ìš” ê¸°ëŠ¥ - 1ì°¨ + 2ì°¨ ì •ì œ í†µí•©]
    1. 1ì°¨ ì •ì œ (Scene Detection): í”„ë ˆì„ ê°„ í”½ì…€ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ì¥ë©´ ì „í™˜ ì‹œì  ê°ì§€
       - threshold: ì¥ë©´ ì „í™˜ìœ¼ë¡œ íŒë‹¨í•˜ê¸° ìœ„í•œ ìµœì†Œ diff_score
    2. 2ì°¨ ì •ì œ (Deduplication): ì €ì¥ ì „ ë§ˆì§€ë§‰ í”„ë ˆì„ê³¼ ë¹„êµí•˜ì—¬ ì¤‘ë³µ ì œê±°
       - dedupe_threshold: ì €ì¥í•˜ê¸° ìœ„í•œ ìµœì†Œ ì´ë¯¸ì§€ ì°¨ì´
    3. ë§ˆìš°ìŠ¤ ì œê±° (Temporal Median): ì—¬ëŸ¬ í”„ë ˆì„ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ë§ˆìš°ìŠ¤ í¬ì¸í„° ì œê±°
    4. ë©”íƒ€ë°ì´í„° ìƒì„±: ê° ì¶”ì¶œ ì‹œì ì˜ ì ìˆ˜ì™€ ì¸ë±ìŠ¤ë¥¼ ê¸°ë¡
    """
    def __init__(self):
        # ì´ˆê¸°í™” ì‹œ íŠ¹ë³„í•œ ìƒíƒœ ì €ì¥ì´ í•„ìš”í•˜ì§€ ì•ŠìŒ
        pass

    def extract_keyframes(self, video_path, output_dir='captured_frames', threshold=30, min_interval=2.0, verbose=False, video_name=None, return_analysis_data=False, dedupe_threshold=10.0):
        """
        [í•µì‹¬ ê¸°ëŠ¥] ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í•˜ì—¬ ì¥ë©´ ì „í™˜ ì‹œì ì˜ ê¹¨ë—í•œ í‚¤í”„ë ˆì„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        1ì°¨ + 2ì°¨ ì •ì œë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ ì¤‘ë³µ ì´ë¯¸ì§€ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        
        Args:
            video_path (str): ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_dir (str): ì¶”ì¶œëœ ì´ë¯¸ì§€ì™€ ë©”íƒ€ë°ì´í„°ê°€ ì €ì¥ë  í´ë” (ê¸°ë³¸ê°’: 'captured_frames')
            threshold (float): ì¥ë©´ ì „í™˜ ê°ì§€ë¥¼ ìœ„í•œ diff_score ì„ê³„ê°’. 
                               í”„ë ˆì„ ê°„ í‰ê·  í”½ì…€ ì°¨ì´ê°€ ì´ ê°’ì„ ì´ˆê³¼í•˜ë©´ ì¥ë©´ ì „í™˜ìœ¼ë¡œ íŒë‹¨.
                               (ê¸°ë³¸ê°’: 30, ë‚®ì„ìˆ˜ë¡ ë¯¼ê°í•˜ê²Œ ê°ì§€)
            min_interval (float): ìº¡ì²˜ ê°„ ìµœì†Œ ì‹œê°„ ê°„ê²©, ì´ˆ ë‹¨ìœ„ (ê¸°ë³¸ê°’: 2.0)
            verbose (bool): ìƒì„¸ ë¶„ì„ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            return_analysis_data (bool): Trueë©´ ì‹œê°í™”ìš© ë¶„ì„ ë°ì´í„°ë„ í•¨ê»˜ ë°˜í™˜ (ê¸°ë³¸ê°’: False)
            dedupe_threshold (float): 2ì°¨ ì •ì œ ì„ê³„ê°’. ë§ˆì§€ë§‰ ì €ì¥ëœ í”„ë ˆì„ê³¼ í”½ì…€ ì°¨ì´ê°€ ì´ ê°’ ì´ìƒì´ì–´ì•¼ ì €ì¥.
                                       (ê¸°ë³¸ê°’: 10.0, 0ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì¤‘ë³µ ê²€ì‚¬ ë¹„í™œì„±í™”)
            
        Returns:
            list: ì¶”ì¶œëœ í”„ë ˆì„ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (timestamp_ms, frame_index, file_name, diff_score í¬í•¨)
            ë˜ëŠ” return_analysis_data=Trueì¸ ê²½ìš°: (metadata, diff_scores, fps) íŠœí”Œ
        """
        if not os.path.exists(video_path):
            print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return ([], [], 0) if return_analysis_data else []

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ: {output_dir}")

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print("âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return ([], [], 0) if return_analysis_data else []

        # ë¹„ë””ì˜¤ ê¸°ë³¸ ì •ë³´ íšë“
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        
        print(f"ğŸ¬ ë¹„ë””ì˜¤ ì •ë³´: {duration:.2f}ì´ˆ, {fps:.2f} fps, {total_frames} í”„ë ˆì„")
        print(f"âš™ï¸ ì„¤ì •ê°’: ì„ê³„ê°’={threshold}, ìµœì†Œ ê°„ê²©={min_interval}ì´ˆ, 2ì°¨ ì •ì œ ì„ê³„ê°’={dedupe_threshold}")

        keyframes_metadata = [] # ìµœì¢… ë°˜í™˜í•  ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        diff_scores_list = []   # ì‹œê°í™”ë¥¼ ìœ„í•œ diff score ìˆ˜ì§‘ ë¦¬ìŠ¤íŠ¸
        prev_frame_gray = None  # ì´ì „ í”„ë ˆì„ ì €ì¥ìš© (ë¹„êµ ëª©ì )
        last_capture_time = -min_interval # ì¤‘ë³µ ìº¡ì²˜ ë°©ì§€ìš© íƒ€ì„ìŠ¤íƒ¬í”„
        
        if video_name is None:
            video_name = os.path.splitext(os.path.basename(video_path))[0]
        
        slide_idx = 1           # ìŠ¬ë¼ì´ë“œ ìˆœë²ˆ (1ë¶€í„° ì‹œì‘)
        debug_idx = 1           # ë””ë²„ê·¸ ì´ë¯¸ì§€ ìˆœë²ˆ
        last_scene_change = 0.0 # ë§ˆì§€ë§‰ ì¥ë©´ ì „í™˜ ì‹œì 
        last_saved_frame = None # 2ì°¨ ì •ì œìš©: ë§ˆì§€ë§‰ ì €ì¥ëœ í”„ë ˆì„ (ì¤‘ë³µ ë¹„êµìš©)
        skipped_count = 0       # ìŠ¤í‚µëœ í”„ë ˆì„ ìˆ˜
        detected_count = 0      # ê°ì§€ëœ ì¥ë©´ ì „í™˜ ìˆ˜
        
        frame_idx = 0

        # ë©”ì¸ ë£¨í”„: ë¹„ë””ì˜¤ì˜ ëª¨ë“  í”„ë ˆì„ì„ ìˆœì°¨ì ìœ¼ë¡œ ì½ìŒ
        while True:
            ret, frame = cap.read()
            if not ret: # ë¹„ë””ì˜¤ ëì— ë„ë‹¬
                break

            current_time = frame_idx / fps # í˜„ì¬ ì‹œì  (ì´ˆ)
            
            # [Step 1] ì²« ë²ˆì§¸ í”„ë ˆì„ íŠ¹ìˆ˜ ì²˜ë¦¬ (ì‹œì‘ ì§€ì )
            if frame_idx == 0:
                last_scene_change = 0.0
                
                # ì²« ìŠ¬ë¼ì´ë“œëŠ” 0~6ì´ˆ êµ¬ê°„ì„ ìƒ˜í”Œë§í•˜ì—¬ ë§ˆìš°ìŠ¤ë¥¼ ì œê±°í•¨
                clean_frame = self._apply_temporal_median_multipoint(
                    cap, 0.0, min(6.0, duration), fps, num_samples=30
                )
                
                # ì €ì¥ ë° ë©”íƒ€ë°ì´í„° ê¸°ë¡
                save_frame = clean_frame if clean_frame is not None else frame
                self._save_frame_with_meta(
                    save_frame, current_time, frame_idx, output_dir, 
                    keyframes_metadata, slide_idx, diff_score=0.0, prefix=video_name
                )
                
                slide_idx += 1
                last_capture_time = current_time
                last_saved_frame = save_frame  # 2ì°¨ ì •ì œìš© ì¶”ì 
                # ë¹„êµë¥¼ ìœ„í•´ í˜„ì¬ í”„ë ˆì„ì„ í‘ë°±/ë¦¬ì‚¬ì´ì¦ˆí•˜ì—¬ ì €ì¥
                prev_frame_gray = cv2.cvtColor(cv2.resize(frame, (640, 360)), cv2.COLOR_BGR2GRAY)
                frame_idx += 1
                continue

            # [Step 2] ì‹œê°„ ê°„ê²© í•„í„°ë§
            # ë„ˆë¬´ ì§§ì€ ì‹œê°„ì— ì—¬ëŸ¬ ë²ˆ ê°ì§€ë˜ëŠ” í˜„ìƒ ë°©ì§€
            if current_time - last_capture_time < min_interval:
                frame_idx += 1
                continue

            # [Step 3] í”½ì…€ ì°¨ì´ ê³„ì‚°ì„ í†µí•œ ì¥ë©´ ì „í™˜ ê°ì§€
            # ì—°ì‚°ëŸ‰ ê°ì†Œë¥¼ ìœ„í•´ 640x360ìœ¼ë¡œ ì¤„ì—¬ì„œ ë¹„êµ
            curr_frame_small = cv2.resize(frame, (640, 360))
            curr_frame_gray = cv2.cvtColor(curr_frame_small, cv2.COLOR_BGR2GRAY)

            # ì´ì „ í”„ë ˆì„ê³¼ í˜„ì¬ í”„ë ˆì„ì˜ ì ˆëŒ€ ì°¨ì´ í•©ê³„ì˜ í‰ê·  ê³„ì‚°
            diff = cv2.absdiff(curr_frame_gray, prev_frame_gray)
            mean_diff = np.mean(diff)
            
            # ì‹œê°í™”ë¥¼ ìœ„í•œ diff score ì €ì¥ (ìƒ˜í”Œë§ + ì¥ë©´ ì „í™˜ ì‹œì  í¬í•¨)
            if frame_idx % 5 == 0 or mean_diff > threshold:
                diff_scores_list.append((frame_idx, float(mean_diff)))

            # ë””ë²„ê¹… ë¡œê·¸ ì¶œë ¥ (ì„ê³„ê°’ì˜ ì ˆë°˜ ì´ìƒ ë³€í™” ì‹œ í‘œì‹œ)
            if verbose and mean_diff > (threshold / 2):
                print(f"   [ë¶„ì„] ì‹œê°„: {current_time:.2f}s | ì°¨ì´: {mean_diff:.2f} (ì„ê³„ê°’: {threshold})")

            # [Step 4] ì¥ë©´ ì „í™˜ í™•ì • ë° ì²˜ë¦¬
            if mean_diff > threshold:
                detected_count += 1
                print(f"ğŸ“¸ ì¥ë©´ ì „í™˜ ê°ì§€: {current_time:.2f}s (ì°¨ì´ ì ìˆ˜: {mean_diff:.2f})")
                
                # ë””ë²„ê·¸ ì´ë¯¸ì§€ ì €ì¥ (ê°ì§€ëœ ì›ë³¸ ìƒíƒœ ê¸°ë¡)
                debug_dir = os.path.join(output_dir, "debug_scene_changes")
                os.makedirs(debug_dir, exist_ok=True)
                self._save_debug_frame(frame, current_time, debug_idx, mean_diff, debug_dir, prefix=video_name)
                debug_idx += 1
                
                # ìŠ¬ë¼ì´ë“œ êµ¬ê°„ ì •ë³´ ê³„ì‚°
                slide_start = last_scene_change
                slide_end = current_time
                slide_duration = slide_end - slide_start
                
                # [ë§ˆìš°ìŠ¤ ì œê±° ì•Œê³ ë¦¬ì¦˜ ì ìš©]
                if slide_duration >= 3.0:
                    # ê¸´ ìŠ¬ë¼ì´ë“œ: ì „ì²´ êµ¬ê°„ì—ì„œ 50ê°œ í”„ë ˆì„ ëœë¤ ìƒ˜í”Œë§ (ê°€ì¥ ê¹¨ë—í•¨)
                    clean_frame = self._apply_temporal_median_multipoint(
                        cap, slide_start, slide_end, fps, num_samples=50
                    )
                else:
                    # ì§§ì€ ìŠ¬ë¼ì´ë“œ: ê°ì§€ ì‹œì  ì „í›„ êµ¬ê°„ì„ ì§‘ì¤‘ ìˆ˜ì§‘
                    current_pos = cap.get(cv2.CAP_PROP_POS_FRAMES)
                    clean_frame = self._apply_temporal_median_bidirectional(
                        cap, current_pos, before_duration=2.0, after_duration=4.0, fps=fps
                    )
                
                save_frame = clean_frame if clean_frame is not None else frame
                
                # [2ì°¨ ì •ì œ] ë§ˆì§€ë§‰ ì €ì¥ëœ í”„ë ˆì„ê³¼ ë¹„êµí•˜ì—¬ ì¤‘ë³µ ê²€ì‚¬
                should_save = True
                image_diff = 0.0
                
                if last_saved_frame is not None and dedupe_threshold > 0:
                    # ì €ì¥ëœ í”„ë ˆì„ê³¼ í˜„ì¬ í”„ë ˆì„ ë¹„êµ
                    saved_gray = cv2.cvtColor(cv2.resize(last_saved_frame, (640, 360)), cv2.COLOR_BGR2GRAY)
                    current_gray = cv2.cvtColor(cv2.resize(save_frame, (640, 360)), cv2.COLOR_BGR2GRAY)
                    diff_img = cv2.absdiff(saved_gray, current_gray)
                    image_diff = np.mean(diff_img)
                    
                    if image_diff < dedupe_threshold:
                        should_save = False
                        skipped_count += 1
                        print(f"   â­ï¸ ìŠ¤í‚µ (ì´ë¯¸ì§€ diff={image_diff:.2f} < {dedupe_threshold})")
                
                if should_save:
                    # ìµœì¢… ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ ì €ì¥ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    self._save_frame_with_meta(
                        save_frame, current_time, frame_idx, output_dir, 
                        keyframes_metadata, slide_idx, diff_score=mean_diff, prefix=video_name
                    )
                    
                    slide_idx += 1
                    last_saved_frame = save_frame  # 2ì°¨ ì •ì œìš© ì¶”ì 
                    print(f"   âœ… ì €ì¥ë¨ (scene{slide_idx-1}, ì´ë¯¸ì§€ diff={image_diff:.2f})")
                
                # ë‹¤ìŒ ê°ì§€ë¥¼ ìœ„í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
                last_capture_time = current_time
                last_scene_change = current_time
                prev_frame_gray = curr_frame_gray
            
            frame_idx += 1

        cap.release()
        
        # [ìƒíƒœ ì €ì¥] Grid Search ë“± ì™¸ë¶€ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ì €ì¥
        self.last_detected_count = detected_count
        self.last_skipped_count = skipped_count
        
        # [ê²°ê³¼ ìš”ì•½]
        print(f"\nğŸ” ê°ì§€ëœ ì¥ë©´ ì „í™˜: {detected_count}ê°œ")
        print(f"â­ï¸ ìŠ¤í‚µëœ í”„ë ˆì„ (2ì°¨ ì •ì œ): {skipped_count}ê°œ")
        print(f"âœ… ìµœì¢… ì €ì¥ëœ ìŠ¬ë¼ì´ë“œ: {len(keyframes_metadata)}ê°œ")
        
        if return_analysis_data:
            return keyframes_metadata, diff_scores_list, fps
        return keyframes_metadata

    def _format_time(self, seconds):
        """ì´ˆ ë‹¨ìœ„ ì‹œê°„ì„ 00h00m00s000ms í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        ms = int((seconds % 1) * 1000)
        total_seconds = int(seconds)
        hours = total_seconds // 3600
        minutes = (total_seconds % 3600) // 60
        secs = total_seconds % 60
        return f"{hours:02d}h{minutes:02d}m{secs:02d}s{ms:03d}ms"

    def _save_frame_with_meta(self, frame, seconds, frame_idx, output_dir, meta_list, slide_idx, diff_score, prefix):
        """
        í”„ë ˆì„ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  í™•ì¥ëœ ë©”íƒ€ë°ì´í„° í˜•ì‹ì„ ë¦¬ìŠ¤íŠ¸ì— ê¸°ë¡í•©ë‹ˆë‹¤.
        íŒŒì¼ëª… í˜•ì‹: {ì˜ìƒíŒŒì¼ëª…}_{ìº¡ì²˜ì¸ë±ìŠ¤}_{h_m_s_ms}_{diff:.2f}.jpg
        """
        time_str = self._format_time(seconds)
        file_name = f"{prefix}_{slide_idx:03d}_{time_str}_{diff_score:.2f}.jpg"
        file_path = os.path.join(output_dir, file_name)
        
        cv2.imwrite(file_path, frame)
        
        meta_list.append({
            "timestamp_ms": int(seconds * 1000),
            "timestamp_human": time_str,
            "frame_index": frame_idx,
            "file_name": file_name,
            "diff_score": round(float(diff_score), 2)
        })

    def _save_debug_frame(self, frame, seconds, idx, diff, debug_dir, prefix):
        """ë””ë²„ê·¸ìš© ì›ë³¸ í”„ë ˆì„ ì €ì¥ (íŒŒì¼ëª…ì— ì ìˆ˜ í¬í•¨)"""
        time_str = self._format_time(seconds)
        file_name = f"debug_{prefix}_{idx:03d}_{time_str}_diff{diff:.2f}.jpg"
        cv2.imwrite(os.path.join(debug_dir, file_name), frame)

    def _apply_temporal_median_multipoint(self, cap, start_time, end_time, fps, num_samples=50):
        """
        [Temporal Median ì•Œê³ ë¦¬ì¦˜] 
        ì§€ì •ëœ ì‹œê°„ ë²”ìœ„ ë‚´ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì—¬ëŸ¬ í”„ë ˆì„ì„ ë½‘ì•„ ì¤‘ì•™ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        ì´ ê³¼ì •ì„ í†µí•´ ì¼ì‹œì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ë§ˆìš°ìŠ¤ í¬ì¸í„°ëŠ” ì œê±°ë˜ê³  ê³ ì •ëœ ë°°ê²½ë§Œ ë‚¨ê²Œ ë©ë‹ˆë‹¤.
        """
        frames = []
        original_pos = cap.get(cv2.CAP_PROP_POS_FRAMES)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        start_frame = int(start_time * fps)
        end_frame = int(end_time * fps)
        
        start_frame = max(0, start_frame)
        end_frame = min(total_frames, end_frame)
        
        if end_frame - start_frame < 5:
            return None
        
        # ë¬´ì‘ìœ„ ìƒ˜í”Œë§ ìœ„ì¹˜ ê²°ì •
        np.random.seed(42)
        random_indices = np.random.randint(start_frame, end_frame, num_samples)
        random_indices = np.unique(random_indices)
        random_indices.sort()
        
        for pos in random_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, pos)
            ret, frame = cap.read()
            if ret:
                frames.append(frame)
        
        cap.set(cv2.CAP_PROP_POS_FRAMES, original_pos) # ì›ë˜ ìœ„ì¹˜ ë³µêµ¬
        
        if len(frames) < 3:
            return None
            
        # í”½ì…€ë³„ ì¤‘ì•™ê°’ ê³„ì‚°
        stacked = np.stack(frames, axis=0)
        return np.median(stacked, axis=0).astype(dtype=np.uint8)

    def _apply_temporal_median_bidirectional(self, cap, start_pos, before_duration=2.0, after_duration=4.0, fps=30.0):
        """ì „í™˜ ì‹œì  ì „í›„ êµ¬ê°„ì—ì„œ í”„ë ˆì„ì„ ìˆ˜ì§‘í•˜ì—¬ ì¤‘ì•™ê°’ ê³„ì‚°"""
        frames = []
        original_pos = cap.get(cv2.CAP_PROP_POS_FRAMES)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        collect_start = max(0, int(start_pos) - int(before_duration * fps))
        collect_end = min(total_frames, int(start_pos) + int(after_duration * fps))
        
        cap.set(cv2.CAP_PROP_POS_FRAMES, collect_start)
        for i in range(collect_start, collect_end, 2): # 2í”„ë ˆì„ ê°„ê²© ìˆ˜ì§‘
            ret, frame = cap.read()
            if not ret: break
            frames.append(frame)
        
        cap.set(cv2.CAP_PROP_POS_FRAMES, original_pos)
        
        if len(frames) < 3: return None
        stacked = np.stack(frames, axis=0)
        return np.median(stacked, axis=0).astype(dtype=np.uint8)

    def _remove_duplicates_by_dhash(self, metadata_list, hash_threshold=5):
        """dHash ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì ìœ¼ë¡œ ì¤‘ë³µëœ ìŠ¬ë¼ì´ë“œ ì œê±°"""
        if not metadata_list: return []

        unique_list = []
        last_hash = None
        
        # ê° ê²°ê³¼ í´ë”ì—ì„œ íŒŒì¼ì„ ë‹¤ì‹œ ì½ì–´ í•´ì‹œ ë¹„êµ
        # (ì•ì„  ê³¼ì •ì—ì„œ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ í•„ìš”)
        for item in metadata_list:
            # output_dir ì •ë³´ë¥¼ ì „ë‹¬ë°›ì§€ ì•Šìœ¼ë¯€ë¡œ íŒŒì¼ëª…ìœ¼ë¡œ ì¬êµ¬ì„± í•„ìš” 
            # (ì´ í•¨ìˆ˜ëŠ” extract_keyframes ë‚´ë¶€ì—ì„œë§Œ ì“°ì´ë¯€ë¡œ ë¡œì»¬ ë³€ìˆ˜ í™œìš© ê°€ëŠ¥)
            pass 
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” extract_keyframes ë‚´ë¶€ì˜ output_dir í™œìš©
        return metadata_list # (êµ¬ì¡° ìœ ì§€ë¥¼ ìœ„í•´ ìš°ì„  ë¦¬í„´, ì‹¤ì œ ì¤‘ë³µì œê±° ë¡œì§ì€ ì¶”ì¶œ ë‹¨ê³„ì—ì„œ ì´ë¯¸ ì¶©ë¶„í•¨)

    def _calculate_dhash(self, image):
        """ì´ë¯¸ì§€ì˜ ì‹œê°ì  íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ëŠ” 64ë¹„íŠ¸ í•´ì‹œ ìƒì„±"""
        resized = cv2.resize(image, (9, 8))
        gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)
        hash_val = 0
        for row in range(8):
            for col in range(8):
                if gray[row, col] < gray[row, col+1]:
                    hash_val |= 1 << (row * 8 + col)
        return hash_val
